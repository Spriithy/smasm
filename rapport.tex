\documentclass[14pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{listings}
\usepackage[french]{babel}
\author{Théophile Dano, Vincent Gouteux, Adrien Legros}
\title{Projet Architecture des Ordinateurs \\ Machine à Pile}

\begin{document}
	\maketitle
	\newpage
	
	\part{Le compilateur assembleur}
	
	Liste des fichiers:
	
	\begin{lstlisting}[frame=simple,language=Bash]
 ast.c
 ast.h
 compiler.h
 grammar.y
 makefile
 scanner.l
 symtable.c
 symtable.h
	\end{lstlisting}

	\section{Approche générale}

	Dans la manière de concevoir ce petit compilateur, nous avons voulu utiliser les manières plus populaires pour le faire. C'est-à-dire créer un compilateur en plusieurs passes, qui toutes ont leur propres entrées/sorties et qui travaillent à la chaîne.

	L'idée est que le travail du compilateur est divisé en 4 phases :
	
	\begin{itemize}
		\item L'analyse lexical (le "lexing")  
		\item L'analyse grammaticale (le "parsing")
		\item L'optimisation
		\item La génération de code
	\end{itemize}

	Nous avons décider d'ignorer la phase d'optimisation du code puisqu'elle n'est pas du tout requise pour ce projet, même si cela pourrait être intéressant.
	
	\subsection{Analyse Lexicale}

	Le rôle du \textit{lexer} est de générer depuis une chaîne de caractères une liste de \textit{jetons} ("\textit{token}" dans la suite) pour identifier chacune des suites de caractères qu'il reconnait.
	
	Le but du lexer est donc de découper ce qui lui est donné en une liste de tokens pour ensuite pouvoir en comprendre la structure.
	
	Regardons sur un petit exemple. Disons que l'on veuille analyser une chaine de charactères qui représente une phrase en français, et que notre lexer ne reconnaisse que les \textit{VERBE}s et les \textit{NOM}s et que tout autre mot soit identifié par un token de type \textit{MOT}.	
	$$ $$
	\textit{Le chat mange la souris}

	Si nous devions exécuter notre lexer sur cette phrase test, il nous reverrait la liste de tokens :
	
	\begin{itemize}
		\item MOT : "Le"
		\item NOM : "chat"
		\item VERBE : "mange"
		\item MOT : "la"
		\item NOM : "souris"
	\end{itemize}

	A ce stade le lexer a fini son travail. Il a atteint la fin de l'entrée et n'a rencontré aucune erreur. Il passe donc la main au parser.
	
	\subsection{Analyse grammaticale}
	
	\paragraph{Grammaire}
	
	L'enjeu d'un parser est, étant donné une chaine de tokens et leur contenu, vérifier qu'ils forment une structure conforme à une grammaire définie à l'avance. Ici il faut entendre le mot grammaire au sens classique du terme, c'est-à-dire une liste de règles définissant comment on peu organiser des mots d'un langage.
	
	Si maintenant on introduit pour notre lexer le token ARCTICLE alors les mots "le" et "la" seront reconnus comme tels. Cela va nous aider à définir une grammaire extrêmement simple avec une unique règle PHRASE-SIMPLE.\\
	 
	Le lexing de notre première phrase se transforme alors en :
	
	\begin{itemize}
		\item ARTICLE : "Le"
		\item NOM : "chat"
		\item VERBE : "mange"
		\item ARTICLE : "la"
		\item NOM : "souris"
	\end{itemize}

	En suivant la notation EBNF (\textit{Extended Backus-Naur Form}) pour les grammaires, on peut définir PHRASE-SIMPLE comme :

	\begin{lstlisting}

PHRASE-SIMPLE
	:
	| ARTICLE NOM ARTICLE NOM
	;

	\end{lstlisting}

	On remarque immédiatement que n'importe quelle phrase basée sur cette structure grammaticale sera reconnue et validée par le parser. (par exemple "\textit{Les taupes creusent le sol}" est une entrée valide).\\
	
	C'est aussi le rôle du parser de faire remonter les possibles erreurs de grammaires contenues dans le programme.

	\paragraph{Représentation interne du code}

	Il est évident que l'on ne veut pas juste vérifier que la structure du code soit correcte mais bien en faire quelque chose. Par exemple générer du code correspondant dans un autre langage, ou colorer les mots clés dans un éditeur etc.
	
	Pour cela, il nous faut garder en mémoire chaque groupe de tokens qui a été reconnu et validé par le parser dans une structure de données appropriée. Les arbres sont pour cela bien adaptés puisqu'ils permettent d'organiser les informations de manière très pratique. On peut représenter une PHRASE-SIMPLE par un nœud ayant pour fils chacun des tokens repérés dans l'ordre.
	
	De manière générale, on utilse un \textit{A.S.T.} ("\textit{Abstract Syntax Tree}" où Arbre de Syntaxe Abstraite).
	\newpage
	
	\subsection{Génération du code}
	
	Cette dernière passe d'un compilateur n'est généralement pas compliquée. En effet, une approche simple et efficace est de parcourir simplement l'arbre (AST) généré par le parser et pour chaque nœud de traverser ses fils récursivement.
	
	Il y a évidement des problèmes qui se posent comme la gestion des identifiants (noms de variables etc.). Sont-ils utilisés dans un cadre correct ? Ont-ils bien étés définis avant ? Si oui, sont-ils du type attendu et ainsi de suite.\\
	
	Ces problèmes se résolvent souvent facilement par l'inspection d'une table de symboles ("\textit{Symtable}" en anglais). Cette table (souvent une \textit{Hash Map}) répertorie chaque noms de symboles définis dans un programme et leur associe des informations connexes (type, mutabilité, définition, etc.).
	
	\section{Application au projet}
	
	Dans cette partie nous allons voir dans quelle mesure il est possible d'appliquer ces techniques au projet en C à l'aide de bibliothèques et d'un \textit{parser-generator} : le tandem \textit{Flex/Bison}, version retravaillée de \textit{Lex/Yacc}.
	
	\subsection{Lexer -- Flex}
	
	Pour la première passe de notre compilateur, le lexing, nous avons choisit d'utiliser Flex un générateur de lexer simple a prendre en main, souvent utilisé dans les cours d'introductions à la compilation et à l'analyse de textes divers.
	
	Nous n'allons pas détailler le fonctionnement interne de Flex mais simplement analyser quelques portions du fichier \textbf{scanner.l}.\\
	
	Dans la première partie, nous avons détaillé comment on pouvait définir certains types de tokens. Il s'agit ici (avec Flex) de le faire avec du code C. Regardons le plus simple des tokens du projet : le mot clé \textbf{pop}.
	
	\begin{lstlisting}[frame=simple,language=C]
pop { return TOKEN_POP; }
	\end{lstlisting}
	
	La syntaxe utilisée n'est pas du tout pensée pour être du C pur. Seul les parties entre accolades doivent être du code C. Ici, le mot \textbf{pop} est utilisé pour désigner un \textit{pattern} a reconnaitre et l'action associée est tout simplement de renvoyer au parser l'information que l'on est tombé sur un token de type POP.
	
	Utiliser Flex est d'une trivialité sans pareil. Cependant, pour identifier un type particulier d'entrée qui n'est pas définit clairement au préalable, il faut utiliser un outil obscur au premier regard, mais qui est très logique.\\
	
	Les expressions régulières ("\textit{regex}" de "\textit{regular expressions}") sont très utilisées en informatique pour décrire des \textit{pattern}s et reconnaitre des formations syntaxiques particulières. On appelle cela plus couramment le \textit{pattern matching}, dans le sens ou l'on essaie de reconnaitre des patterns particuliers.\\
	
	Prenons l'exemple des étiquettes (labels) d'instructions dans notre petit langage assembleur. On ne sait pas donner une liste de mots étant à reconnaitre comme des étiquettes et donc on ne peux pas lister tous les labels possibles. Cependant, on connait la manière dont sont définis les labels. Ce sont des mots (combinaisons de caractères ASCII suivis de chiffres ou d'underscores) qui ne sont pas des mots-clés et qui sont suivis de deux points.
	
	On peut alors définir ca comme suit à l'aide des expressions régulières et Flex :
	
	\begin{lstlisting}[frame=simple,language=C]
[a-zA-Z_][a-zA-Z0-9_]*":" {
	yytext[strlen(yytext) - 1] = 0; // pour ignorer les ":"
	yylval.ident = strdup(yytext);
	
	return TOKEN_LBL;
}
	\end{lstlisting}
	
	Sans rentrer dans les détails des expression régulières, la règle utilisée ici est très simple. La règle [a-zA-Z\_] précise que le premier caractère doit être une lettre peu importe sa casse, ou bien un underscore. Ensuite, [a-zA-Z0-9\_]* permet de récupérer n'importe quel caractère alphanumérique ou underscore. L'étoile est utilisée pour indiquer que l'on accepte une répétition sans limite de cette règle. Enfin, ":" indique que le token doit se terminer par deux points.\\
	
	Finalement, si la règle est "matchée", on exécute quelques actions et on retourne le token au parser qui saura quoi en faire.
	
	\subsection{Parser -- Bison}
	
	Maintenant que notre fichier ou code source a été relu et découpé par le lexer, le parser peut commencer son travail. C'est-à-dire construire l'AST et vérifier que l'entrée soit conforme à la grammaire utilisée.\\
	
	Ici, nous n'allons pas nous éterniser sur le pourquoi du comment des grammaires mais il est important de noter qu'il faut faire attention aux récursions des règles (une règle s'invoque elle même, par la droite ou la gauche par exemple) car cela peut mener à des boucles infinies et planter le programme.\\
	
	Cela étant dit, regardons comment on peut mettre en place une grammaire et son parser avec Bison. Nous allons regarder qu'un seul exemple pour pouvoir ensuite parler de la génération de l'AST. Dans le fichier \textbf{grammar.y} on trouve une liste de règles de grammaire parmis lesquelles celle qui définit comment un programme assembleur doit être organisé.\\
	
	\begin{lstlisting}[frame=simple,language=C]
program
  : %empty
  | stat NL program
  | TOKEN_LBL {
       tok = $1;
       if (yydolog) fprintf(yylog, "%3d: label -> %s\n", pc, $1);
       add_sym_entry(labels, $1, pc);
  } stat NL program
  | NL program
  ;
	\end{lstlisting}
	
	
	Comme pour Flex, les commandes en langage C sont définies entre accolades. Ici, NL correspond a un retour à la ligne, stat est n'importe quelle instruction assembleur valide, et TOKEN\_LBL le token associé aux labels (vu plus haut). \\
	
	La règle \textbf{\%empty} signifie qu'un programme valide peut ne rien être (chaine vide). Sinon (la barre verticale signifie une alternative), un label suivi d'une instruction et d'un retour à la ligne est accepté, une instruction simple et un retour à la ligne de même.
	
	On peut observer que la règle \textbf{program} est récursive (par la droite). C'est-à-dire qu'une suite de \textbf{program} est un \textbf{program} (eux mêmes constitués d'instruction et d'étiquettes). \\
	
	Regardons maintenant l'action faite lorsque l'on rencontre un label. On récupère le token dans la variable \textbf{tok} grâce au symbole \textbf{\$1} qui sera remplacé par Bison par le pointeur adapté. Ensuite on ajoute le label à la Symtable (table des symboles) en indiquant comme information connexe son adresse dans le code. \\
	
	En jetant un rapide coup d'œil à la règle \textbf{stat},
	
		\begin{lstlisting}[frame=simple,language=C]
stat
  : TOKEN_POP INT {
      tok = $1;
      if (yydolog) fprintf(yylog, "%3d: pop %d\n", pc, $2);
      ast_tail->next = new_instr(POP, $2, pc++);
      ast_tail = ast_tail->next;
  | ...
  ;
}
	\end{lstlisting}
	
	On voit comment est gérée la génération de l'AST par le parser. Lorsqu'une instruction correcte est rencontrée, on génère un nœud dans l'arbre qui lui correspond. Ici, l'instruction \textbf{pop} est valide si le token POP est suivi du token INT (pour un nombre entier). Dans ce cas, on génère un simple nœud instruction avec son adresse dans le code et son argument (le nombre entier).
	
	\newpage
	
	\subsection{AST et génération de code}
	
	Dans cette dernière partie liée au compilateur, nous allons nous intéresser à l'AST et la génération de code pour la machine virtuelle (à pile) qui sera discutée dans la prochaine partie.\\
	
	Pour cela, intéressons-nous à la structure utilisée pour les nœuds de l'AST.
	
	\begin{lstlisting}[frame=simple,language=C]
struct node {
	int lno;
	int sym;
	union {
		int arg;
		char *name;
	};
	struct node *next;
};
	\end{lstlisting}
	
	On remarque immédiatement qu'il s'agit en réalité d'une liste chaînée et non d'un arbre (même si une liste chaînée est un arbre ...). En effet, comme le langage assembleur utilisé a un syntaxe très linéaire, il n'y a aucune structure de code qui soit assez complexe pour justifier l'utilisation de plusieurs fils par nœuds (si l'on avait pensé le langage en acceptant de simples expressions, on aurait du modifier cette structure).\\
	
	Le membre \textbf{int} \textit{lno} correspond à la ligne ou l'instruction apparait. \textbf{int} \textit{sym} dénote du type de l'instruction (pop, push, call, etc.). L'\textbf{union} est utilisée pour donner l'information de soit l'argument numérique de l'instruction, ou l'identifiant (par exemple dans \textbf{jmp} \textit{label}).\\
	
	\paragraph{Pour construire l'AST}
	On utilise ensuite les trois fonctions déclarées dans le fichier \textbf{ast.h}:
	
	\begin{lstlisting}[frame=simple,language=C]	
struct node *new_ctrl(int sym, char *to, int lno);
struct node *new_instr(int sym, int arg, int lno);
struct node *new_ph(int sym, char *name, int lno);
	\end{lstlisting}	
	
	qui servent respectivement à :
	\begin{itemize}
		\item Ajouter un nœud de contrôle (\textbf{jmp, call} ...)
		\item Ajouter un nœud pour une instruction classique (\textbf{pop} 10, ...)
		\item Ajouter un "\textit{placeholder}" pour signaler la position d'un label (ignoré à l'étape de la génération de code, mais utilisé pour repérer un label dans le code).
	\end{itemize}
	
	\newpage
	
	\part{La machine a pile}
	
	\newpage
	
	\part{Exemples d'exécution}
	
\end{document}